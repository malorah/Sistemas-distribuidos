# 1) Bajar y limpiar contenedores y volúmenes huérfanos
docker-compose -f docker-compose-hadoop.yml -f docker-compose.yml down --remove-orphans -v

# 2) (Opcional) Limpiar datos persistentes si quieres un arranque 100% limpio
rm -rf ./mongo-data
rm -rf ./hdfs-output

# 3) Levantar MongoDB + Redis + Storage-API
docker-compose -f docker-compose-hadoop.yml -f docker-compose.yml up -d mongodb cache storage-api

# 4) Inicializar colecciones en Mongo (solo la primera vez o si borraste ./mongo-data)
docker exec -it storage-api npm run init

# 5) Levantar el scraper para poblar datos en MongoDB
docker-compose -f docker-compose-hadoop.yml -f docker-compose.yml up -d --build --force-recreate scraper

# 6) (Verifica que el scraper esté insertando en MongoDB):
docker-compose -f docker-compose-hadoop.yml -f docker-compose.yml logs -f waze-scraper

# 7) Levantar Hadoop + ETL + Pig con todas las correcciones
docker-compose -f docker-compose-hadoop.yml -f docker-compose.yml up -d --build --force-recreate namenode datanode resourcemanager nodemanager etl pig

# 8) Verificar logs del ETL (debería subir un CSV a /input en HDFS)
docker-compose -f docker-compose-hadoop.yml -f docker-compose.yml logs -f etl

# 9) Forzar reinicio de Pig para procesar cualquier CSV recién subido
docker-compose -f docker-compose-hadoop.yml -f docker-compose.yml restart pig
docker-compose -f docker-compose-hadoop.yml -f docker-compose.yml logs -f pig

# 10) Verificar que los resultados estén en ./hdfs-output
ls -lR ./hdfs-output
